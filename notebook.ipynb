{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 30px; color:CadetBlue;\">MATHEMATICAL MODELLING IN MACHINE LEARNING - 30554</p>\n",
    "<p style= \"text-align: center; font-size: 18px;\">Luca Raffo</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:CadetBlue\">Abstract</span>\n",
    "In this upcoming notebook, we're gearing up to dive into the process of training two different classifiers.\n",
    "The dataset we will work on comprises 1000 samples, each endowed with a collection of 35 features. Each sample has a label, an integer ranging between 0 and 3 (extremes included).  \n",
    "In particular, we aim to embark on a comparative study of the performances of two renowned algorithms: k-NearestNeighbors and Random Forest.  \n",
    "We will optimize the hyperparameters for both of these models, in order to exploit their full potential.  \n",
    "In the end we will conclusively determine which classifier exhibited a better performance on this specific dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:CadetBlue\">Exploratory Data Analysis</span>\n",
    "We start our journey by importing the libraries which are fundamental in order to work on the dataset, and the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib tk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.201113</td>\n",
       "      <td>-0.290211</td>\n",
       "      <td>-1.293273</td>\n",
       "      <td>-0.987445</td>\n",
       "      <td>0.375111</td>\n",
       "      <td>3.262836</td>\n",
       "      <td>-1.221763</td>\n",
       "      <td>-0.036009</td>\n",
       "      <td>1.435816</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.220223</td>\n",
       "      <td>1.230786</td>\n",
       "      <td>2.145584</td>\n",
       "      <td>1.564664</td>\n",
       "      <td>0.584401</td>\n",
       "      <td>1.494508</td>\n",
       "      <td>0.287226</td>\n",
       "      <td>-0.852132</td>\n",
       "      <td>-0.759382</td>\n",
       "      <td>1.041190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.628621</td>\n",
       "      <td>2.803231</td>\n",
       "      <td>-0.671938</td>\n",
       "      <td>0.328820</td>\n",
       "      <td>-0.074892</td>\n",
       "      <td>-6.121306</td>\n",
       "      <td>0.736208</td>\n",
       "      <td>-0.902092</td>\n",
       "      <td>-1.411483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.623831</td>\n",
       "      <td>2.929491</td>\n",
       "      <td>0.070468</td>\n",
       "      <td>-3.152713</td>\n",
       "      <td>-4.343053</td>\n",
       "      <td>0.909593</td>\n",
       "      <td>-1.577434</td>\n",
       "      <td>1.009611</td>\n",
       "      <td>-0.621972</td>\n",
       "      <td>-0.587301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.100785</td>\n",
       "      <td>-1.583855</td>\n",
       "      <td>5.478388</td>\n",
       "      <td>-0.468925</td>\n",
       "      <td>-0.803414</td>\n",
       "      <td>-1.751841</td>\n",
       "      <td>-1.017715</td>\n",
       "      <td>-1.547837</td>\n",
       "      <td>-0.120226</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.087988</td>\n",
       "      <td>-4.678151</td>\n",
       "      <td>-3.911785</td>\n",
       "      <td>-6.108772</td>\n",
       "      <td>-3.052242</td>\n",
       "      <td>-0.027595</td>\n",
       "      <td>-0.809724</td>\n",
       "      <td>3.607353</td>\n",
       "      <td>-0.021959</td>\n",
       "      <td>-2.211963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.940043</td>\n",
       "      <td>-0.837257</td>\n",
       "      <td>-0.181058</td>\n",
       "      <td>-1.814802</td>\n",
       "      <td>1.129368</td>\n",
       "      <td>2.175010</td>\n",
       "      <td>-1.678725</td>\n",
       "      <td>1.800978</td>\n",
       "      <td>-0.716211</td>\n",
       "      <td>...</td>\n",
       "      <td>2.812262</td>\n",
       "      <td>2.075730</td>\n",
       "      <td>1.488613</td>\n",
       "      <td>-0.842421</td>\n",
       "      <td>-1.413351</td>\n",
       "      <td>0.405909</td>\n",
       "      <td>1.508635</td>\n",
       "      <td>-0.783645</td>\n",
       "      <td>-0.593335</td>\n",
       "      <td>-0.363084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.068997</td>\n",
       "      <td>-2.446645</td>\n",
       "      <td>2.682663</td>\n",
       "      <td>1.958176</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>-0.289787</td>\n",
       "      <td>0.797141</td>\n",
       "      <td>0.106172</td>\n",
       "      <td>-0.531375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801879</td>\n",
       "      <td>-1.485274</td>\n",
       "      <td>1.815136</td>\n",
       "      <td>1.434652</td>\n",
       "      <td>-0.734058</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>-1.585892</td>\n",
       "      <td>2.955094</td>\n",
       "      <td>-1.617037</td>\n",
       "      <td>0.730129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.412984</td>\n",
       "      <td>-3.669425</td>\n",
       "      <td>2.356743</td>\n",
       "      <td>0.467361</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>2.799420</td>\n",
       "      <td>1.007144</td>\n",
       "      <td>0.520349</td>\n",
       "      <td>-0.934891</td>\n",
       "      <td>...</td>\n",
       "      <td>1.261325</td>\n",
       "      <td>2.595839</td>\n",
       "      <td>2.017200</td>\n",
       "      <td>3.008557</td>\n",
       "      <td>1.816542</td>\n",
       "      <td>1.891215</td>\n",
       "      <td>1.422110</td>\n",
       "      <td>-1.145137</td>\n",
       "      <td>-0.016489</td>\n",
       "      <td>-0.584580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2</td>\n",
       "      <td>1.157384</td>\n",
       "      <td>-0.190413</td>\n",
       "      <td>1.928101</td>\n",
       "      <td>3.214276</td>\n",
       "      <td>0.997408</td>\n",
       "      <td>-1.338326</td>\n",
       "      <td>-0.267054</td>\n",
       "      <td>-0.815329</td>\n",
       "      <td>-1.114050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371433</td>\n",
       "      <td>1.287553</td>\n",
       "      <td>2.317670</td>\n",
       "      <td>0.195089</td>\n",
       "      <td>0.148927</td>\n",
       "      <td>-0.251898</td>\n",
       "      <td>-1.338197</td>\n",
       "      <td>1.244734</td>\n",
       "      <td>1.060938</td>\n",
       "      <td>-0.544084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>0.080442</td>\n",
       "      <td>-0.265915</td>\n",
       "      <td>1.015759</td>\n",
       "      <td>-0.338928</td>\n",
       "      <td>1.215722</td>\n",
       "      <td>-2.106663</td>\n",
       "      <td>-0.650065</td>\n",
       "      <td>-0.264707</td>\n",
       "      <td>...</td>\n",
       "      <td>2.219610</td>\n",
       "      <td>-0.724935</td>\n",
       "      <td>7.981604</td>\n",
       "      <td>3.304423</td>\n",
       "      <td>-3.472791</td>\n",
       "      <td>-0.034423</td>\n",
       "      <td>-0.780662</td>\n",
       "      <td>5.539539</td>\n",
       "      <td>-1.054519</td>\n",
       "      <td>4.222095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3</td>\n",
       "      <td>2.042390</td>\n",
       "      <td>-0.818479</td>\n",
       "      <td>2.949888</td>\n",
       "      <td>-0.625669</td>\n",
       "      <td>-0.218483</td>\n",
       "      <td>0.520263</td>\n",
       "      <td>-0.928446</td>\n",
       "      <td>0.134972</td>\n",
       "      <td>0.881317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521867</td>\n",
       "      <td>0.524603</td>\n",
       "      <td>-1.532325</td>\n",
       "      <td>-3.241046</td>\n",
       "      <td>-0.836606</td>\n",
       "      <td>1.029299</td>\n",
       "      <td>-0.468110</td>\n",
       "      <td>0.059098</td>\n",
       "      <td>-0.744620</td>\n",
       "      <td>-1.327555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>0.243249</td>\n",
       "      <td>1.924340</td>\n",
       "      <td>1.131938</td>\n",
       "      <td>-4.181031</td>\n",
       "      <td>-0.629542</td>\n",
       "      <td>2.481535</td>\n",
       "      <td>1.198435</td>\n",
       "      <td>1.425699</td>\n",
       "      <td>0.335587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536029</td>\n",
       "      <td>2.957825</td>\n",
       "      <td>-1.683065</td>\n",
       "      <td>0.300637</td>\n",
       "      <td>1.056681</td>\n",
       "      <td>1.072130</td>\n",
       "      <td>-1.823992</td>\n",
       "      <td>-1.606653</td>\n",
       "      <td>-0.238929</td>\n",
       "      <td>2.177328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0        1   0.201113  -0.290211  -1.293273  -0.987445   0.375111   3.262836   \n",
       "1        2   1.628621   2.803231  -0.671938   0.328820  -0.074892  -6.121306   \n",
       "2        0   1.100785  -1.583855   5.478388  -0.468925  -0.803414  -1.751841   \n",
       "3        2   0.940043  -0.837257  -0.181058  -1.814802   1.129368   2.175010   \n",
       "4        2  -0.068997  -2.446645   2.682663   1.958176   0.013928  -0.289787   \n",
       "..     ...        ...        ...        ...        ...        ...        ...   \n",
       "995      0  -0.412984  -3.669425   2.356743   0.467361   0.110527   2.799420   \n",
       "996      2   1.157384  -0.190413   1.928101   3.214276   0.997408  -1.338326   \n",
       "997      3  -0.419290   0.080442  -0.265915   1.015759  -0.338928   1.215722   \n",
       "998      3   2.042390  -0.818479   2.949888  -0.625669  -0.218483   0.520263   \n",
       "999      1   0.243249   1.924340   1.131938  -4.181031  -0.629542   2.481535   \n",
       "\n",
       "     feature_7  feature_8  feature_9  ...  feature_26  feature_27  feature_28  \\\n",
       "0    -1.221763  -0.036009   1.435816  ...   -1.220223    1.230786    2.145584   \n",
       "1     0.736208  -0.902092  -1.411483  ...   -0.623831    2.929491    0.070468   \n",
       "2    -1.017715  -1.547837  -0.120226  ...   -1.087988   -4.678151   -3.911785   \n",
       "3    -1.678725   1.800978  -0.716211  ...    2.812262    2.075730    1.488613   \n",
       "4     0.797141   0.106172  -0.531375  ...    0.801879   -1.485274    1.815136   \n",
       "..         ...        ...        ...  ...         ...         ...         ...   \n",
       "995   1.007144   0.520349  -0.934891  ...    1.261325    2.595839    2.017200   \n",
       "996  -0.267054  -0.815329  -1.114050  ...   -0.371433    1.287553    2.317670   \n",
       "997  -2.106663  -0.650065  -0.264707  ...    2.219610   -0.724935    7.981604   \n",
       "998  -0.928446   0.134972   0.881317  ...   -0.521867    0.524603   -1.532325   \n",
       "999   1.198435   1.425699   0.335587  ...    0.536029    2.957825   -1.683065   \n",
       "\n",
       "     feature_29  feature_30  feature_31  feature_32  feature_33  feature_34  \\\n",
       "0      1.564664    0.584401    1.494508    0.287226   -0.852132   -0.759382   \n",
       "1     -3.152713   -4.343053    0.909593   -1.577434    1.009611   -0.621972   \n",
       "2     -6.108772   -3.052242   -0.027595   -0.809724    3.607353   -0.021959   \n",
       "3     -0.842421   -1.413351    0.405909    1.508635   -0.783645   -0.593335   \n",
       "4      1.434652   -0.734058    0.779631   -1.585892    2.955094   -1.617037   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "995    3.008557    1.816542    1.891215    1.422110   -1.145137   -0.016489   \n",
       "996    0.195089    0.148927   -0.251898   -1.338197    1.244734    1.060938   \n",
       "997    3.304423   -3.472791   -0.034423   -0.780662    5.539539   -1.054519   \n",
       "998   -3.241046   -0.836606    1.029299   -0.468110    0.059098   -0.744620   \n",
       "999    0.300637    1.056681    1.072130   -1.823992   -1.606653   -0.238929   \n",
       "\n",
       "     feature_35  \n",
       "0      1.041190  \n",
       "1     -0.587301  \n",
       "2     -2.211963  \n",
       "3     -0.363084  \n",
       "4      0.730129  \n",
       "..          ...  \n",
       "995   -0.584580  \n",
       "996   -0.544084  \n",
       "997    4.222095  \n",
       "998   -1.327555  \n",
       "999    2.177328  \n",
       "\n",
       "[1000 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv('dataset.csv', index_col=0)\n",
    "\n",
    "# Visualize the dataset\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search for missing values in the dataset, we do this by using the pandas DataFrame methods 'isna()' and 'sum()'.  \n",
    "In particular:  \n",
    "1. 'df.isna()' returns a DataFrame of the same shape as df, but with True at places where the original DataFrame has NaN (or missing) values, and False where it doesn't.    \n",
    "\n",
    "2. 'df.isna().sum(): The sum() function when applied after isna() counts the number of True values along each column of the DataFrame returned by isna(). This effectively gives you the total number of missing values in each column.    \n",
    "\n",
    "3. 'df.isna().sum().sum(): The second sum() function then adds up the sums of missing values from all columns, giving you the total number of missing values in the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_missing_values = df.isna().sum().sum()\n",
    "total_missing_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a quick look at the dataset we soon notice that the first column shows the label of each datapoint. We therefore proceed to remove this column from the dataset and save it in a separate list, order to be able to process the features directly from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df['label']\n",
    "df_nolabel = df.drop('label', axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.282933</td>\n",
       "      <td>-0.134222</td>\n",
       "      <td>0.425977</td>\n",
       "      <td>0.104823</td>\n",
       "      <td>0.115679</td>\n",
       "      <td>0.413091</td>\n",
       "      <td>-0.039085</td>\n",
       "      <td>0.090042</td>\n",
       "      <td>0.054637</td>\n",
       "      <td>0.579426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133214</td>\n",
       "      <td>0.474120</td>\n",
       "      <td>0.686559</td>\n",
       "      <td>0.295006</td>\n",
       "      <td>-0.065978</td>\n",
       "      <td>0.154925</td>\n",
       "      <td>0.133279</td>\n",
       "      <td>0.297240</td>\n",
       "      <td>0.051504</td>\n",
       "      <td>0.307107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.818610</td>\n",
       "      <td>1.960321</td>\n",
       "      <td>2.928722</td>\n",
       "      <td>1.966148</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>2.594393</td>\n",
       "      <td>1.810147</td>\n",
       "      <td>1.004531</td>\n",
       "      <td>1.015530</td>\n",
       "      <td>3.410613</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000638</td>\n",
       "      <td>1.850237</td>\n",
       "      <td>3.627684</td>\n",
       "      <td>3.218812</td>\n",
       "      <td>1.898731</td>\n",
       "      <td>1.009853</td>\n",
       "      <td>1.022389</td>\n",
       "      <td>2.179414</td>\n",
       "      <td>0.959773</td>\n",
       "      <td>1.828522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.767465</td>\n",
       "      <td>-8.091172</td>\n",
       "      <td>-8.976704</td>\n",
       "      <td>-5.834941</td>\n",
       "      <td>-2.869279</td>\n",
       "      <td>-7.531196</td>\n",
       "      <td>-6.957603</td>\n",
       "      <td>-2.943063</td>\n",
       "      <td>-2.846600</td>\n",
       "      <td>-12.305623</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.882318</td>\n",
       "      <td>-5.431653</td>\n",
       "      <td>-10.873214</td>\n",
       "      <td>-12.435061</td>\n",
       "      <td>-5.892671</td>\n",
       "      <td>-2.966331</td>\n",
       "      <td>-2.886916</td>\n",
       "      <td>-6.412861</td>\n",
       "      <td>-3.257298</td>\n",
       "      <td>-6.405617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.850264</td>\n",
       "      <td>-1.403214</td>\n",
       "      <td>-1.563170</td>\n",
       "      <td>-1.248238</td>\n",
       "      <td>-0.596544</td>\n",
       "      <td>-1.305197</td>\n",
       "      <td>-1.316516</td>\n",
       "      <td>-0.619754</td>\n",
       "      <td>-0.680124</td>\n",
       "      <td>-1.690841</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.564726</td>\n",
       "      <td>-0.796360</td>\n",
       "      <td>-1.552170</td>\n",
       "      <td>-1.728238</td>\n",
       "      <td>-1.381113</td>\n",
       "      <td>-0.547456</td>\n",
       "      <td>-0.549123</td>\n",
       "      <td>-1.156720</td>\n",
       "      <td>-0.611668</td>\n",
       "      <td>-0.883565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.371561</td>\n",
       "      <td>-0.119651</td>\n",
       "      <td>0.490831</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>0.130476</td>\n",
       "      <td>0.171989</td>\n",
       "      <td>0.061171</td>\n",
       "      <td>0.067529</td>\n",
       "      <td>0.054134</td>\n",
       "      <td>0.630859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108615</td>\n",
       "      <td>0.541051</td>\n",
       "      <td>0.699022</td>\n",
       "      <td>0.439118</td>\n",
       "      <td>-0.065264</td>\n",
       "      <td>0.123897</td>\n",
       "      <td>0.139922</td>\n",
       "      <td>0.375740</td>\n",
       "      <td>0.068088</td>\n",
       "      <td>0.440117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.554514</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>2.517349</td>\n",
       "      <td>1.396477</td>\n",
       "      <td>0.834385</td>\n",
       "      <td>1.991070</td>\n",
       "      <td>1.254006</td>\n",
       "      <td>0.754327</td>\n",
       "      <td>0.758453</td>\n",
       "      <td>3.054894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785315</td>\n",
       "      <td>1.741152</td>\n",
       "      <td>3.141299</td>\n",
       "      <td>2.487484</td>\n",
       "      <td>1.215040</td>\n",
       "      <td>0.875205</td>\n",
       "      <td>0.794936</td>\n",
       "      <td>1.879065</td>\n",
       "      <td>0.657521</td>\n",
       "      <td>1.571504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.186449</td>\n",
       "      <td>6.958862</td>\n",
       "      <td>9.334812</td>\n",
       "      <td>6.197449</td>\n",
       "      <td>3.222567</td>\n",
       "      <td>8.363020</td>\n",
       "      <td>5.155523</td>\n",
       "      <td>3.343691</td>\n",
       "      <td>2.757545</td>\n",
       "      <td>11.341444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.964663</td>\n",
       "      <td>6.420313</td>\n",
       "      <td>12.267814</td>\n",
       "      <td>9.117051</td>\n",
       "      <td>7.860133</td>\n",
       "      <td>2.867358</td>\n",
       "      <td>3.132727</td>\n",
       "      <td>6.167547</td>\n",
       "      <td>3.081773</td>\n",
       "      <td>6.903088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_1    feature_2    feature_3    feature_4    feature_5  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.282933    -0.134222     0.425977     0.104823     0.115679   \n",
       "std       1.818610     1.960321     2.928722     1.966148     0.999951   \n",
       "min      -6.767465    -8.091172    -8.976704    -5.834941    -2.869279   \n",
       "25%      -0.850264    -1.403214    -1.563170    -1.248238    -0.596544   \n",
       "50%       0.371561    -0.119651     0.490831    -0.012048     0.130476   \n",
       "75%       1.554514     1.147522     2.517349     1.396477     0.834385   \n",
       "max       5.186449     6.958862     9.334812     6.197449     3.222567   \n",
       "\n",
       "         feature_6    feature_7    feature_8    feature_9   feature_10  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean      0.413091    -0.039085     0.090042     0.054637     0.579426  ...   \n",
       "std       2.594393     1.810147     1.004531     1.015530     3.410613  ...   \n",
       "min      -7.531196    -6.957603    -2.943063    -2.846600   -12.305623  ...   \n",
       "25%      -1.305197    -1.316516    -0.619754    -0.680124    -1.690841  ...   \n",
       "50%       0.171989     0.061171     0.067529     0.054134     0.630859  ...   \n",
       "75%       1.991070     1.254006     0.754327     0.758453     3.054894  ...   \n",
       "max       8.363020     5.155523     3.343691     2.757545    11.341444  ...   \n",
       "\n",
       "        feature_26   feature_27   feature_28   feature_29   feature_30  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.133214     0.474120     0.686559     0.295006    -0.065978   \n",
       "std       1.000638     1.850237     3.627684     3.218812     1.898731   \n",
       "min      -2.882318    -5.431653   -10.873214   -12.435061    -5.892671   \n",
       "25%      -0.564726    -0.796360    -1.552170    -1.728238    -1.381113   \n",
       "50%       0.108615     0.541051     0.699022     0.439118    -0.065264   \n",
       "75%       0.785315     1.741152     3.141299     2.487484     1.215040   \n",
       "max       2.964663     6.420313    12.267814     9.117051     7.860133   \n",
       "\n",
       "        feature_31   feature_32   feature_33   feature_34   feature_35  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean      0.154925     0.133279     0.297240     0.051504     0.307107  \n",
       "std       1.009853     1.022389     2.179414     0.959773     1.828522  \n",
       "min      -2.966331    -2.886916    -6.412861    -3.257298    -6.405617  \n",
       "25%      -0.547456    -0.549123    -1.156720    -0.611668    -0.883565  \n",
       "50%       0.123897     0.139922     0.375740     0.068088     0.440117  \n",
       "75%       0.875205     0.794936     1.879065     0.657521     1.571504  \n",
       "max       2.867358     3.132727     6.167547     3.081773     6.903088  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nolabel.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to become more familiar with the dataset, we plot some interesting graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "df.hist(figsize=(20,20), bins=20)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the occurrence of label 0 is slightly above 200 instances, along with labels 1, 2, and 3. Hence, the data indicates a relatively low level of variation among them.\n",
    "From the histograms it seems that each feature is distributed pretty symmetrically, furthermore they resemble the shapes of some gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "df_nolabel.boxplot(figsize=(10,6))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots seem to confirm our supposition, they are symmetrical and resemble those of some gaussians.\n",
    "Let's check statistically if our supposition is true: first with some QQ-plots, then with some Shapiro tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ-plots\n",
    "num_rows = 5\n",
    "num_cols = 7\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < df_nolabel.shape[1]:\n",
    "        stats.probplot(df_nolabel.iloc[:, i], dist=\"norm\", plot=ax)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature number 1 is not normally distributed\n",
      "feature number 2 is normally distributed\n",
      "feature number 3 is normally distributed\n",
      "feature number 4 is normally distributed\n",
      "feature number 5 is normally distributed\n",
      "feature number 6 is not normally distributed\n",
      "feature number 7 is not normally distributed\n",
      "feature number 8 is normally distributed\n",
      "feature number 9 is normally distributed\n",
      "feature number 10 is normally distributed\n",
      "feature number 11 is not normally distributed\n",
      "feature number 12 is not normally distributed\n",
      "feature number 13 is normally distributed\n",
      "feature number 14 is normally distributed\n",
      "feature number 15 is not normally distributed\n",
      "feature number 16 is normally distributed\n",
      "feature number 17 is normally distributed\n",
      "feature number 18 is not normally distributed\n",
      "feature number 19 is normally distributed\n",
      "feature number 20 is normally distributed\n",
      "feature number 21 is normally distributed\n",
      "feature number 22 is normally distributed\n",
      "feature number 23 is normally distributed\n",
      "feature number 24 is normally distributed\n",
      "feature number 25 is normally distributed\n",
      "feature number 26 is normally distributed\n",
      "feature number 27 is normally distributed\n",
      "feature number 28 is normally distributed\n",
      "feature number 29 is not normally distributed\n",
      "feature number 30 is normally distributed\n",
      "feature number 31 is normally distributed\n",
      "feature number 32 is normally distributed\n",
      "feature number 33 is not normally distributed\n",
      "feature number 34 is normally distributed\n",
      "feature number 35 is not normally distributed\n",
      "\n",
      " the number of features normally distributed is 25\n"
     ]
    }
   ],
   "source": [
    "# Shapiro tests\n",
    "count = 0\n",
    "for i in range(df_nolabel.shape[1]):\n",
    "    stat, p_value = shapiro(df_nolabel.iloc[:, i])\n",
    "    alpha = 0.05\n",
    "    if p_value > alpha:\n",
    "        print(\"feature number {} is normally distributed\".format(i+1))\n",
    "        count = count + 1\n",
    "    else:\n",
    "        print(\"feature number {} is not normally distributed\".format(i+1))\n",
    "print(\"\\n the number of features normally distributed is {}\".format(count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro test is very sensible and every small fluctuation could result in a false test, even if the data is normally distributed. In general we are satisfied with the result, every feature resembles a gaussian in the QQ-plots, some more than others.  \n",
    "\n",
    "Let's take a look at the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between features\n",
    "corr = df_nolabel.corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
    "plt.title('Correlations between features')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features seem pretty independent between themselves. We could argue that the dataset is a sample from an independent multivariate normal distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:CadetBlue\">Normalization (and PCA visualization)</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize our features. This is important because later on we will use K-NearestNeighbors classifier, that is very sensitive to scaled data. Random forests are an ensemble method built upon decision trees, that are indifferent to scaling, so we can keep the data normalized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exploration of our dataset, we've leveraged boxplots as an analytical tool to visually examine the distributions of our features. From these visualizations, it's evident that the distributions display symmetry, which suggests a Gaussian trend. Outliers, represented by points beyond the 'whiskers' of the boxplot, are also present but they are symmetrically scattered and relatively close to the mean.\n",
    "\n",
    "In terms of data preprocessing, this understanding of our data guides us towards the choice of a scaler. Given the observations, using the StandardScaler is a fitting choice for our dataset. The StandardScaler works effectively when the data is normally distributed and outliers are not extreme, as it standardizes features by removing the mean and scaling to unit variance. This technique helps preserve the essence of the original distributions, which aligns with our goal of capturing as much information as possible from the data.\n",
    "\n",
    "On the other hand, the RobustScaler is more suitable for datasets with extreme outliers and non-symmetric distributions, as it scales features using statistics that are robust to outliers. In our case, however, the outliers are not extreme and the distributions are symmetric, which makes the StandardScaler a more appropriate option.\n",
    "\n",
    "Hence, by choosing the StandardScaler, we're not only considering the nature of our data but also preserving crucial information embedded within our original distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045013</td>\n",
       "      <td>-0.079613</td>\n",
       "      <td>-0.587324</td>\n",
       "      <td>-0.555815</td>\n",
       "      <td>0.259575</td>\n",
       "      <td>1.098974</td>\n",
       "      <td>-0.653687</td>\n",
       "      <td>-0.125545</td>\n",
       "      <td>1.360738</td>\n",
       "      <td>0.312672</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.353251</td>\n",
       "      <td>0.409161</td>\n",
       "      <td>0.402393</td>\n",
       "      <td>0.394647</td>\n",
       "      <td>0.342705</td>\n",
       "      <td>1.327176</td>\n",
       "      <td>0.150651</td>\n",
       "      <td>-0.527640</td>\n",
       "      <td>-0.845296</td>\n",
       "      <td>0.401664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.740324</td>\n",
       "      <td>1.499205</td>\n",
       "      <td>-0.375066</td>\n",
       "      <td>0.113984</td>\n",
       "      <td>-0.190675</td>\n",
       "      <td>-2.519922</td>\n",
       "      <td>0.428518</td>\n",
       "      <td>-0.988153</td>\n",
       "      <td>-1.444422</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756941</td>\n",
       "      <td>1.327721</td>\n",
       "      <td>-0.169915</td>\n",
       "      <td>-1.071651</td>\n",
       "      <td>-2.253723</td>\n",
       "      <td>0.747678</td>\n",
       "      <td>-1.674088</td>\n",
       "      <td>0.327027</td>\n",
       "      <td>-0.702056</td>\n",
       "      <td>-0.489387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.449938</td>\n",
       "      <td>-0.739857</td>\n",
       "      <td>1.725988</td>\n",
       "      <td>-0.291959</td>\n",
       "      <td>-0.919597</td>\n",
       "      <td>-0.834884</td>\n",
       "      <td>-0.540906</td>\n",
       "      <td>-1.631307</td>\n",
       "      <td>-0.172275</td>\n",
       "      <td>-1.196385</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221034</td>\n",
       "      <td>-2.786048</td>\n",
       "      <td>-1.268204</td>\n",
       "      <td>-1.990480</td>\n",
       "      <td>-1.573555</td>\n",
       "      <td>-0.180830</td>\n",
       "      <td>-0.922814</td>\n",
       "      <td>1.519568</td>\n",
       "      <td>-0.076581</td>\n",
       "      <td>-1.378343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.361506</td>\n",
       "      <td>-0.358812</td>\n",
       "      <td>-0.207373</td>\n",
       "      <td>-0.976827</td>\n",
       "      <td>1.014246</td>\n",
       "      <td>0.679465</td>\n",
       "      <td>-0.906258</td>\n",
       "      <td>1.704072</td>\n",
       "      <td>-0.759440</td>\n",
       "      <td>0.715088</td>\n",
       "      <td>...</td>\n",
       "      <td>2.678680</td>\n",
       "      <td>0.866057</td>\n",
       "      <td>0.221203</td>\n",
       "      <td>-0.353545</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.248659</td>\n",
       "      <td>1.345911</td>\n",
       "      <td>-0.496200</td>\n",
       "      <td>-0.672203</td>\n",
       "      <td>-0.366704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.193613</td>\n",
       "      <td>-1.180204</td>\n",
       "      <td>0.770921</td>\n",
       "      <td>0.943104</td>\n",
       "      <td>-0.101806</td>\n",
       "      <td>-0.271058</td>\n",
       "      <td>0.462197</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.577339</td>\n",
       "      <td>0.483573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668573</td>\n",
       "      <td>-1.059526</td>\n",
       "      <td>0.311257</td>\n",
       "      <td>0.354235</td>\n",
       "      <td>-0.352032</td>\n",
       "      <td>0.618919</td>\n",
       "      <td>-1.682365</td>\n",
       "      <td>1.220137</td>\n",
       "      <td>-1.739346</td>\n",
       "      <td>0.231463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.382856</td>\n",
       "      <td>-1.804282</td>\n",
       "      <td>0.659582</td>\n",
       "      <td>0.184482</td>\n",
       "      <td>-0.005154</td>\n",
       "      <td>0.920263</td>\n",
       "      <td>0.578270</td>\n",
       "      <td>0.428581</td>\n",
       "      <td>-0.974883</td>\n",
       "      <td>1.377909</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127956</td>\n",
       "      <td>1.147302</td>\n",
       "      <td>0.366985</td>\n",
       "      <td>0.843451</td>\n",
       "      <td>0.991958</td>\n",
       "      <td>1.720209</td>\n",
       "      <td>1.261238</td>\n",
       "      <td>-0.662150</td>\n",
       "      <td>-0.070878</td>\n",
       "      <td>-0.487898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.481075</td>\n",
       "      <td>-0.028678</td>\n",
       "      <td>0.513150</td>\n",
       "      <td>1.582287</td>\n",
       "      <td>0.882214</td>\n",
       "      <td>-0.675416</td>\n",
       "      <td>-0.126003</td>\n",
       "      <td>-0.901738</td>\n",
       "      <td>-1.151391</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504578</td>\n",
       "      <td>0.439857</td>\n",
       "      <td>0.449854</td>\n",
       "      <td>-0.031057</td>\n",
       "      <td>0.113240</td>\n",
       "      <td>-0.403055</td>\n",
       "      <td>-1.439973</td>\n",
       "      <td>0.434964</td>\n",
       "      <td>1.052269</td>\n",
       "      <td>-0.465740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.386325</td>\n",
       "      <td>0.109560</td>\n",
       "      <td>-0.236362</td>\n",
       "      <td>0.463542</td>\n",
       "      <td>-0.454856</td>\n",
       "      <td>0.309526</td>\n",
       "      <td>-1.142788</td>\n",
       "      <td>-0.737137</td>\n",
       "      <td>-0.314618</td>\n",
       "      <td>0.223235</td>\n",
       "      <td>...</td>\n",
       "      <td>2.086109</td>\n",
       "      <td>-0.648379</td>\n",
       "      <td>2.011944</td>\n",
       "      <td>0.935414</td>\n",
       "      <td>-1.795155</td>\n",
       "      <td>-0.187595</td>\n",
       "      <td>-0.894374</td>\n",
       "      <td>2.406574</td>\n",
       "      <td>-1.152958</td>\n",
       "      <td>2.142139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.967957</td>\n",
       "      <td>-0.349228</td>\n",
       "      <td>0.862210</td>\n",
       "      <td>-0.371721</td>\n",
       "      <td>-0.334345</td>\n",
       "      <td>0.041329</td>\n",
       "      <td>-0.491566</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.814445</td>\n",
       "      <td>-0.164423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654992</td>\n",
       "      <td>0.027298</td>\n",
       "      <td>-0.611959</td>\n",
       "      <td>-1.099108</td>\n",
       "      <td>-0.406068</td>\n",
       "      <td>0.866275</td>\n",
       "      <td>-0.588514</td>\n",
       "      <td>-0.109324</td>\n",
       "      <td>-0.829908</td>\n",
       "      <td>-0.894427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.021832</td>\n",
       "      <td>1.050640</td>\n",
       "      <td>0.241168</td>\n",
       "      <td>-2.180914</td>\n",
       "      <td>-0.745630</td>\n",
       "      <td>0.797674</td>\n",
       "      <td>0.683999</td>\n",
       "      <td>1.330298</td>\n",
       "      <td>0.276792</td>\n",
       "      <td>-0.986319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402759</td>\n",
       "      <td>1.343043</td>\n",
       "      <td>-0.653533</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.591564</td>\n",
       "      <td>0.908709</td>\n",
       "      <td>-1.915367</td>\n",
       "      <td>-0.874017</td>\n",
       "      <td>-0.302758</td>\n",
       "      <td>1.023317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.045013 -0.079613 -0.587324 -0.555815  0.259575  1.098974 -0.653687   \n",
       "1    0.740324  1.499205 -0.375066  0.113984 -0.190675 -2.519922  0.428518   \n",
       "2    0.449938 -0.739857  1.725988 -0.291959 -0.919597 -0.834884 -0.540906   \n",
       "3    0.361506 -0.358812 -0.207373 -0.976827  1.014246  0.679465 -0.906258   \n",
       "4   -0.193613 -1.180204  0.770921  0.943104 -0.101806 -0.271058  0.462197   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995 -0.382856 -1.804282  0.659582  0.184482 -0.005154  0.920263  0.578270   \n",
       "996  0.481075 -0.028678  0.513150  1.582287  0.882214 -0.675416 -0.126003   \n",
       "997 -0.386325  0.109560 -0.236362  0.463542 -0.454856  0.309526 -1.142788   \n",
       "998  0.967957 -0.349228  0.862210 -0.371721 -0.334345  0.041329 -0.491566   \n",
       "999 -0.021832  1.050640  0.241168 -2.180914 -0.745630  0.797674  0.683999   \n",
       "\n",
       "           7         8         9   ...        25        26        27  \\\n",
       "0   -0.125545  1.360738  0.312672  ... -1.353251  0.409161  0.402393   \n",
       "1   -0.988153 -1.444422  0.272900  ... -0.756941  1.327721 -0.169915   \n",
       "2   -1.631307 -0.172275 -1.196385  ... -1.221034 -2.786048 -1.268204   \n",
       "3    1.704072 -0.759440  0.715088  ...  2.678680  0.866057  0.221203   \n",
       "4    0.016066 -0.577339  0.483573  ...  0.668573 -1.059526  0.311257   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.428581 -0.974883  1.377909  ...  1.127956  1.147302  0.366985   \n",
       "996 -0.901738 -1.151391  0.093618  ... -0.504578  0.439857  0.449854   \n",
       "997 -0.737137 -0.314618  0.223235  ...  2.086109 -0.648379  2.011944   \n",
       "998  0.044750  0.814445 -0.164423  ... -0.654992  0.027298 -0.611959   \n",
       "999  1.330298  0.276792 -0.986319  ...  0.402759  1.343043 -0.653533   \n",
       "\n",
       "           28        29        30        31        32        33        34  \n",
       "0    0.394647  0.342705  1.327176  0.150651 -0.527640 -0.845296  0.401664  \n",
       "1   -1.071651 -2.253723  0.747678 -1.674088  0.327027 -0.702056 -0.489387  \n",
       "2   -1.990480 -1.573555 -0.180830 -0.922814  1.519568 -0.076581 -1.378343  \n",
       "3   -0.353545 -0.709972  0.248659  1.345911 -0.496200 -0.672203 -0.366704  \n",
       "4    0.354235 -0.352032  0.618919 -1.682365  1.220137 -1.739346  0.231463  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.843451  0.991958  1.720209  1.261238 -0.662150 -0.070878 -0.487898  \n",
       "996 -0.031057  0.113240 -0.403055 -1.439973  0.434964  1.052269 -0.465740  \n",
       "997  0.935414 -1.795155 -0.187595 -0.894374  2.406574 -1.152958  2.142139  \n",
       "998 -1.099108 -0.406068  0.866275 -0.588514 -0.109324 -0.829908 -0.894427  \n",
       "999  0.001750  0.591564  0.908709 -1.915367 -0.874017 -0.302758  1.023317  \n",
       "\n",
       "[1000 rows x 35 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data scaling\n",
    "scaler = StandardScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_nolabel))\n",
    "df_normalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in order to visualize the distribution of the labels geometrically, we perform a PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-components PCA\n",
    "pca = PCA(n_components = 3)\n",
    "principal_components = pca.fit_transform(df_normalized)\n",
    "df_pca = pd.DataFrame(principal_components, columns=['PC1', 'PC2', 'PC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the PCA\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(df_pca['PC1'], df_pca['PC2'], df_pca['PC3'], c=df['label'])\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('PCA: PC1 vs PC2 vs PC3 (Color by Label)')\n",
    "fig.colorbar(scatter, label='Label')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot seems to be messy, it could be due to the fact that we project our datapoints onto a 3-dimensional space, ignoring much information.\n",
    "It could also be due to the fact that our dataset is intrinsically messy, anyway we will keep all the dimensions when training the classifiers since neither should show big computational problems with this small dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:CadetBlue\">Models Training</span>\n",
    "This is the most important part of the notebook. We have decided to divide it in three parts:  \n",
    "<span style=\"color:CadetBlue\">1. </span> Firstly we will train both models without any hyperparameter tuning.  \n",
    "<span style=\"color:CadetBlue\">2. </span> Then, we will train both models using hyperparameters tuning, realized by GridSearch cross-validation on a small range of possibilities in order to keep the notebook computationally feasible, and keeping all the features.  \n",
    "<span style=\"color:CadetBlue\">3. </span> In the end, we will again train both models using hyperparameters tuning, realized by RandomizedSearch on a bigger range of possibilities, this time including the number of features as a hyperparameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is wise to split the dataset in X_train and X_test (as well as the labels), so that we will train our classifiers on X_train, and evaluate their performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_normalized, label, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:CadetBlue\">1. No Hyperparameter Tuning</span>\n",
    "For the time being, we just train the two classifiers without hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48\n"
     ]
    }
   ],
   "source": [
    "# K-NN without tuning\n",
    "knn1 = KNeighborsClassifier()\n",
    "knn1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn1.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6233333333333333\n"
     ]
    }
   ],
   "source": [
    "# Random Forest without tuning\n",
    "random_forest1 = RandomForestClassifier()\n",
    "random_forest1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest1.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part shows that without any hyperparameter tuning the Random Forest seems to work better. It is remarkable that if we train them again (with the same seed in train_test_split), the k-NearestNeighbors will always return the same accuracy, because it isn't stochastic. Instead Random Forest will show different results for each run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:CadetBlue\">2. Simple Hyperparameter Tuning</span>\n",
    "No we will tune the hyperparameters with GridSearch (without including the number of features in the hyperparameters)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch searches through all the possible choices of hyperparameters, therefore we keep the number of choices small, to keep the notebook computationally feasible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters involved in the optimization of k-NearestNeighbors are:  \n",
    "1. The number of labeled neighbors considered when deciding which label to assign to a new datapoint.\n",
    "2. The weights, which refer to the method used to weigh or account for the contribution of the neighbors when making predictions. In particular, in 'uniform' all points in each neighborhood are weighted equally. This means that each of the 'k' nearest neighbors has an equal vote in determining the class of the object in question. Instead in 'distance' weights are assigned to the neighbors based on the inverse of their distance. Closer neighbors of a query point will have a greater influence than neighbors which are further away.  \n",
    "3. The metric, that is to say the method we use to calculate the distance between datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for k-NN: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "Accuracy for k-NN: 0.59\n"
     ]
    }
   ],
   "source": [
    "# KNN simple tuning\n",
    "\n",
    "# Creating the classifier\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "# Defining the hyperparameters\n",
    "knn2_parameters = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "# GridSearch and fitting\n",
    "knn2_clf = GridSearchCV(knn1, knn2_parameters, cv=5)\n",
    "knn2_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for k-NN: {knn2_clf.best_params_}\")\n",
    "\n",
    "# Prediction\n",
    "knn2_y_pred = knn2_clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy for k-NN: {accuracy_score(y_test, knn2_y_pred)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice an improvement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to the Random Forest, the hyperparameters involved in the optimization are:  \n",
    "1. The number of estimators, which controls the number of trees in the random forest. Each tree provides a prediction, and the class with the most votes becomes the model's prediction.  \n",
    "2. The max_depth, this parameter represents the depth of each tree in the forest. The deeper the tree, the more splits it has, and it captures more information about the data. However, deeper trees are more complex and can lead to overfitting. (If max_depth = None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples).\n",
    "3. The min_samples_split represents the minimum number of samples required to split an internal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy for Random Forest: 0.6566666666666666\n"
     ]
    }
   ],
   "source": [
    "# Random Forest simple tuning\n",
    "\n",
    "# Creating the classifier\n",
    "random_forest2 = RandomForestClassifier()\n",
    "\n",
    "# Defining the hyperparameters\n",
    "random_forest2_parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearch and fitting\n",
    "random_forest2_clf = GridSearchCV(random_forest1, random_forest2_parameters, cv=5)\n",
    "random_forest2_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for Random Forest: {random_forest2_clf.best_params_}\")\n",
    "\n",
    "# Prediction\n",
    "random_forest2_y_pred = random_forest2_clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy for Random Forest: {accuracy_score(y_test, random_forest2_y_pred)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest still shows a better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:CadetBlue\">3. Advanced Tuning</span>\n",
    "In order to try to improve our accuracy, we can include the number of features in the hyperparameters.  \n",
    "Having a simpler model (i.e. less features) could help to prevent overfitting, resulting in a higher accuracy. Unfortunately, the number of possible subsets of features is exponential in the number of features, so we cannot compute them by hand with a GridSearch, and neither a RandomizedSearch would work well because we would try only a small family of all the possible subsets, so we have to think of another trick.  \n",
    "We create a gerarchy of importance among the features: we can do it thanks to feature_importance_ method implemented in RandomForest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    importance\n",
      "15    0.055997\n",
      "10    0.048311\n",
      "27    0.046381\n",
      "6     0.046024\n",
      "30    0.042589\n",
      "2     0.041187\n",
      "33    0.039565\n",
      "3     0.038801\n",
      "1     0.038776\n",
      "7     0.036338\n",
      "28    0.034627\n",
      "12    0.034595\n",
      "35    0.034283\n",
      "4     0.034146\n",
      "11    0.033566\n",
      "29    0.031052\n",
      "25    0.029261\n",
      "9     0.020967\n",
      "17    0.019812\n",
      "18    0.019247\n",
      "34    0.019204\n",
      "31    0.018917\n",
      "13    0.018735\n",
      "16    0.018705\n",
      "22    0.018525\n",
      "24    0.018468\n",
      "5     0.018458\n",
      "19    0.018454\n",
      "32    0.018413\n",
      "23    0.018408\n",
      "21    0.018219\n",
      "8     0.018085\n",
      "20    0.017702\n",
      "26    0.017256\n",
      "14    0.016928\n"
     ]
    }
   ],
   "source": [
    "# Creating a gerarchy among the features\n",
    "importances = random_forest1.feature_importances_\n",
    "\n",
    "feature_importances = pd.DataFrame(random_forest1.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances.index = feature_importances.index + 1\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(feature_importances.index, feature_importances['importance'], color='CadetBlue', align='center')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "print(feature_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, it is shows that the 15th feature is the most important, the10th feature is the second most important and so on...  \n",
    "We save this gerarchy into a list and move the indices back of one unit (we do it only because of technical issues: most methods in python starts counting from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = feature_importances.index.tolist()\n",
    "important_features = [feat - 1 for feat in important_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our classifiers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use RandomizedSearch cross-validation, so we can choose bigger ranges for our hyperparameters, since the method itself guarantees that the training will remain computationally feasible. In particular, we made the number of possible n_neighbors choices bigger, we introduced two metrics, and a hyperparameter 'p', which only regards the Minkowski metric (it chooses its polynomial degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 10, 27, 6, 30]\n",
      "Best parameters for 5 features: {'weights': 'distance', 'p': 5, 'n_neighbors': 32, 'metric': 'minkowski'}\n",
      "Best accuracy for 5 features: 0.54\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7]\n",
      "Best parameters for 10 features: {'weights': 'distance', 'p': 4, 'n_neighbors': 2, 'metric': 'euclidean'}\n",
      "Best accuracy for 10 features: 0.67\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11]\n",
      "Best parameters for 15 features: {'weights': 'uniform', 'p': 2, 'n_neighbors': 9, 'metric': 'minkowski'}\n",
      "Best accuracy for 15 features: 0.6966666666666667\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18]\n",
      "Best parameters for 20 features: {'weights': 'distance', 'p': 4, 'n_neighbors': 7, 'metric': 'manhattan'}\n",
      "Best accuracy for 20 features: 0.6866666666666666\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18, 34, 31, 13, 16, 22]\n",
      "Best parameters for 25 features: {'weights': 'distance', 'p': 2, 'n_neighbors': 7, 'metric': 'manhattan'}\n",
      "Best accuracy for 25 features: 0.6\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18, 34, 31, 13, 16, 22, 24, 5, 19, 32, 23]\n",
      "Best parameters for 30 features: {'weights': 'distance', 'p': 1, 'n_neighbors': 12, 'metric': 'manhattan'}\n",
      "Best accuracy for 30 features: 0.6066666666666667\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18, 34, 31, 13, 16, 22, 24, 5, 19, 32, 23, 21, 8, 20, 26, 14]\n",
      "Best parameters for 35 features: {'weights': 'distance', 'p': 1, 'n_neighbors': 44, 'metric': 'minkowski'}\n",
      "Best accuracy for 35 features: 0.5766666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN advanced tuning\n",
    "knn3_parameters = {\n",
    "    \"n_neighbors\": range(1, 50),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"],\n",
    "    \"p\": [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# List to store the best accuracy for each set of features\n",
    "knn3_best_accuracies = []\n",
    "\n",
    "# Number of features to consider in each iteration\n",
    "feature_counts = np.arange(5, len(important_features) + 1, 5)\n",
    "\n",
    "knn3_best_accuracy = 0\n",
    "knn3_best_num_features = 0\n",
    "\n",
    "# Loop over the number of features to consider\n",
    "for num_features in feature_counts:\n",
    "\n",
    "    # Select the features\n",
    "    selected_features = important_features[:num_features]\n",
    "    print([feature + 1 for feature in important_features[:num_features]])\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # Create and fit the model\n",
    "    knn3 = KNeighborsClassifier()\n",
    "    clf = RandomizedSearchCV(knn3, knn3_parameters, cv=5)\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    knn3_best_accuracies.append(accuracy)\n",
    "\n",
    "    # Check if this model is better than the previous best\n",
    "    if accuracy > knn3_best_accuracy:\n",
    "        knn3_best_accuracy = accuracy\n",
    "        knn3_best_num_features = num_features\n",
    "        knn3_final = clf\n",
    "\n",
    "    print(f\"Best parameters for {num_features} features: {clf.best_params_}\")\n",
    "    print(f\"Best accuracy for {num_features} features: {accuracy}\\n\")\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(feature_counts, knn3_best_accuracies, marker='o')\n",
    "plt.title('Best k-NN Accuracy vs. Number of Features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Best Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 9, 'metric': 'minkowski'}\n",
      "Number of features in the best model: 15\n",
      "Best accuracy: 0.6966666666666667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model parameters: {knn3_final.best_params_}\")\n",
    "print(f\"Number of features in the best model: {knn3_best_num_features}\")\n",
    "print(f\"Best accuracy: {knn3_best_accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we increase the range of our hyperparameters for Random Forest.  \n",
    "We added min_samples_leaf, which governs the minimum number of samples required to be at a leaf node, and we added max_features, which represents the number of features to consider when looking for the best split:  \n",
    "if 'sqrt': then max_features = sqrt(n_features),  \n",
    "if 'log2': Then max_features = log2(n_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 10, 27, 6, 30]\n",
      "Best parameters for 5 features: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
      "Best accuracy for 5 features: 0.5633333333333334\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7]\n",
      "Best parameters for 10 features: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 40}\n",
      "Best accuracy for 10 features: 0.6433333333333333\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11]\n",
      "Best parameters for 15 features: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10}\n",
      "Best accuracy for 15 features: 0.6766666666666666\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18]\n",
      "Best parameters for 20 features: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
      "Best accuracy for 20 features: 0.69\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18, 34, 31, 13, 16, 22]\n",
      "Best parameters for 25 features: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Best accuracy for 25 features: 0.67\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18, 34, 31, 13, 16, 22, 24, 5, 19, 32, 23]\n",
      "Best parameters for 30 features: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10}\n",
      "Best accuracy for 30 features: 0.6566666666666666\n",
      "\n",
      "[15, 10, 27, 6, 30, 2, 33, 3, 1, 7, 28, 12, 35, 4, 11, 29, 25, 9, 17, 18, 34, 31, 13, 16, 22, 24, 5, 19, 32, 23, 21, 8, 20, 26, 14]\n",
      "Best parameters for 35 features: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
      "Best accuracy for 35 features: 0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest advanced tuning\n",
    "random_forest3_parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 300, 400, 500],  \n",
    "    \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
    "    \"min_samples_split\": [2, 5, 10, 15, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6, 8, 10],\n",
    "    \"max_features\": ['sqrt', 'log2']\n",
    "}\n",
    "# List to store the best accuracy for each set of features\n",
    "best_accuracies = []\n",
    "\n",
    "# Number of features to consider in each iteration\n",
    "feature_counts = np.arange(5, len(important_features) + 1, 5)\n",
    "\n",
    "random_forest3_best_accuracy = 0\n",
    "random_forest3_best_num_features = 0\n",
    "\n",
    "# Loop over the number of features to consider\n",
    "for num_features in feature_counts:\n",
    "\n",
    "    # Select the features\n",
    "    selected_features = important_features[:num_features]\n",
    "    print([feature + 1 for feature in important_features[:num_features]])\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # Create and fit the model\n",
    "    random_forest3 = RandomForestClassifier()\n",
    "    clf = RandomizedSearchCV(random_forest3, random_forest3_parameters, cv=5)\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    best_accuracies.append(accuracy)\n",
    "\n",
    "    # Check if this model is better than the previous best\n",
    "    if accuracy > random_forest3_best_accuracy:\n",
    "        random_forest3_best_accuracy = accuracy\n",
    "        random_forest3_best_num_features = num_features\n",
    "        random_forest3_final = clf\n",
    "\n",
    "    print(f\"Best parameters for {num_features} features: {clf.best_params_}\")\n",
    "    print(f\"Best accuracy for {num_features} features: {accuracy}\\n\")\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(feature_counts, best_accuracies, marker='o')\n",
    "plt.title('Best k-NN Accuracy vs. Number of Features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Best Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
      "Number of features in the best model: 20\n",
      "Best accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model parameters: {random_forest3_final.best_params_}\")\n",
    "print(f\"Number of features in the best model: {random_forest3_best_num_features}\")\n",
    "print(f\"Best accuracy: {random_forest3_best_accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:CadetBlue\">Conclusions and Final Prediction</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial round of model fitting, where no hyperparameter tuning was conducted, Random Forest performed better than KNN. The performance comparison remained the same in the second stage of testing, which involved hyperparameter tuning but did not consider the number of features as a tunable hyperparameter. This can be attributed to the inherent strength of Random Forest, a robust ensemble method that leverages a multitude of decision trees to make predictions. This ensemble approach reduces overfitting and increases the model's generalization ability, thereby leading to improved performance.\n",
    "\n",
    "However, the scenario changed when feature importance was incorporated into the model fitting. Random Forest was used to generate a ranking of feature importance, which was then included as an additional hyperparameter in the tuning phase. With this information, KNN performed slightly better than the Random Forest. The reason for this could be that feature importance provides a form of feature selection. When the most relevant features were emphasized and introduced into the hyperparameter space, the KNN algorithm's performance improved. By focusing on a smaller, more relevant set of features, KNN might have been able to reduce noise in the data and make more accurate predictions.\n",
    "\n",
    "In conclusion, both Random Forest and KNN have demonstrated their strengths in different aspects of this experiment. While Random Forest generally performs better with a larger feature set, KNN seems to excel when feature selection is taken into account. Therefore, it's crucial to remember that the choice of the classifier and its hyperparameters should be informed by the specific characteristics of the data at hand, rather than relying on the assumption that one classifier will universally outperform the other.  \n",
    "\n",
    "It is important to say that given the closeness of the performances of the classifier, and the stochastic nature of Random Forest, the result also depends slightly on the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.259876</td>\n",
       "      <td>1.259865</td>\n",
       "      <td>-0.908350</td>\n",
       "      <td>-1.853991</td>\n",
       "      <td>0.385436</td>\n",
       "      <td>-1.105350</td>\n",
       "      <td>1.991604</td>\n",
       "      <td>-0.975180</td>\n",
       "      <td>-0.610046</td>\n",
       "      <td>-2.077745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582761</td>\n",
       "      <td>-0.758695</td>\n",
       "      <td>-1.753105</td>\n",
       "      <td>0.902077</td>\n",
       "      <td>0.079239</td>\n",
       "      <td>0.731773</td>\n",
       "      <td>2.401233</td>\n",
       "      <td>-0.056351</td>\n",
       "      <td>-1.691496</td>\n",
       "      <td>0.692310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.529047</td>\n",
       "      <td>1.120999</td>\n",
       "      <td>-4.326973</td>\n",
       "      <td>-0.893663</td>\n",
       "      <td>0.598676</td>\n",
       "      <td>-0.352028</td>\n",
       "      <td>-1.222735</td>\n",
       "      <td>-0.078255</td>\n",
       "      <td>-0.574625</td>\n",
       "      <td>0.527005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304697</td>\n",
       "      <td>-0.329206</td>\n",
       "      <td>1.524611</td>\n",
       "      <td>0.662350</td>\n",
       "      <td>-0.898723</td>\n",
       "      <td>2.235237</td>\n",
       "      <td>0.596613</td>\n",
       "      <td>-0.914091</td>\n",
       "      <td>-0.821066</td>\n",
       "      <td>-1.109875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012222</td>\n",
       "      <td>-0.177944</td>\n",
       "      <td>-1.297251</td>\n",
       "      <td>-0.147131</td>\n",
       "      <td>-1.011613</td>\n",
       "      <td>1.365393</td>\n",
       "      <td>-1.620815</td>\n",
       "      <td>-0.499335</td>\n",
       "      <td>1.425400</td>\n",
       "      <td>2.320669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359054</td>\n",
       "      <td>1.573846</td>\n",
       "      <td>2.876398</td>\n",
       "      <td>0.634198</td>\n",
       "      <td>-0.699412</td>\n",
       "      <td>-0.424788</td>\n",
       "      <td>0.146164</td>\n",
       "      <td>-0.335836</td>\n",
       "      <td>-0.473143</td>\n",
       "      <td>-0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089448</td>\n",
       "      <td>-2.826220</td>\n",
       "      <td>3.938348</td>\n",
       "      <td>-1.599376</td>\n",
       "      <td>1.190689</td>\n",
       "      <td>2.446367</td>\n",
       "      <td>1.675493</td>\n",
       "      <td>-0.457380</td>\n",
       "      <td>0.497983</td>\n",
       "      <td>1.533945</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337707</td>\n",
       "      <td>-1.240190</td>\n",
       "      <td>0.553714</td>\n",
       "      <td>1.999368</td>\n",
       "      <td>-0.796620</td>\n",
       "      <td>1.076099</td>\n",
       "      <td>0.907682</td>\n",
       "      <td>2.919312</td>\n",
       "      <td>-0.316218</td>\n",
       "      <td>2.976172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.318715</td>\n",
       "      <td>2.223043</td>\n",
       "      <td>-1.566211</td>\n",
       "      <td>1.067817</td>\n",
       "      <td>1.312906</td>\n",
       "      <td>-1.992862</td>\n",
       "      <td>1.511649</td>\n",
       "      <td>0.590657</td>\n",
       "      <td>-0.708409</td>\n",
       "      <td>-3.091795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455276</td>\n",
       "      <td>-1.605761</td>\n",
       "      <td>-3.269420</td>\n",
       "      <td>-0.527665</td>\n",
       "      <td>1.735746</td>\n",
       "      <td>0.167339</td>\n",
       "      <td>0.612655</td>\n",
       "      <td>-0.788140</td>\n",
       "      <td>-0.006253</td>\n",
       "      <td>0.208321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3.541143</td>\n",
       "      <td>-0.192635</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>1.083794</td>\n",
       "      <td>1.907259</td>\n",
       "      <td>3.304881</td>\n",
       "      <td>-0.295063</td>\n",
       "      <td>-0.435150</td>\n",
       "      <td>2.418466</td>\n",
       "      <td>-0.104594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263075</td>\n",
       "      <td>-0.052187</td>\n",
       "      <td>-2.089612</td>\n",
       "      <td>-0.257640</td>\n",
       "      <td>3.437469</td>\n",
       "      <td>-1.640569</td>\n",
       "      <td>-0.345324</td>\n",
       "      <td>-1.990600</td>\n",
       "      <td>1.105713</td>\n",
       "      <td>1.017189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.791110</td>\n",
       "      <td>1.798410</td>\n",
       "      <td>-1.411891</td>\n",
       "      <td>-0.161851</td>\n",
       "      <td>3.359425</td>\n",
       "      <td>2.244013</td>\n",
       "      <td>-1.182066</td>\n",
       "      <td>-1.770667</td>\n",
       "      <td>-0.640782</td>\n",
       "      <td>-3.740578</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.021918</td>\n",
       "      <td>-1.518739</td>\n",
       "      <td>-0.928221</td>\n",
       "      <td>-0.176392</td>\n",
       "      <td>1.954623</td>\n",
       "      <td>-1.229088</td>\n",
       "      <td>1.287181</td>\n",
       "      <td>-0.600885</td>\n",
       "      <td>-1.487602</td>\n",
       "      <td>1.171236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.473525</td>\n",
       "      <td>-0.185459</td>\n",
       "      <td>0.951580</td>\n",
       "      <td>0.334926</td>\n",
       "      <td>-1.087654</td>\n",
       "      <td>-0.260079</td>\n",
       "      <td>-2.475909</td>\n",
       "      <td>0.493006</td>\n",
       "      <td>0.840025</td>\n",
       "      <td>-0.238076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666892</td>\n",
       "      <td>0.124335</td>\n",
       "      <td>0.803261</td>\n",
       "      <td>-2.884849</td>\n",
       "      <td>-1.499423</td>\n",
       "      <td>-0.120510</td>\n",
       "      <td>0.685016</td>\n",
       "      <td>0.393198</td>\n",
       "      <td>-0.495217</td>\n",
       "      <td>-2.488886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-1.023864</td>\n",
       "      <td>-1.582119</td>\n",
       "      <td>-2.382119</td>\n",
       "      <td>3.359051</td>\n",
       "      <td>0.936478</td>\n",
       "      <td>-1.220890</td>\n",
       "      <td>2.902376</td>\n",
       "      <td>0.158716</td>\n",
       "      <td>-0.128563</td>\n",
       "      <td>6.958211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091385</td>\n",
       "      <td>3.584082</td>\n",
       "      <td>1.706777</td>\n",
       "      <td>4.733215</td>\n",
       "      <td>3.194062</td>\n",
       "      <td>0.854198</td>\n",
       "      <td>0.635615</td>\n",
       "      <td>-3.252377</td>\n",
       "      <td>-1.140647</td>\n",
       "      <td>-1.304214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.091433</td>\n",
       "      <td>-0.777077</td>\n",
       "      <td>2.764774</td>\n",
       "      <td>1.293155</td>\n",
       "      <td>0.918216</td>\n",
       "      <td>-0.588605</td>\n",
       "      <td>-2.711436</td>\n",
       "      <td>2.575381</td>\n",
       "      <td>0.436611</td>\n",
       "      <td>1.012601</td>\n",
       "      <td>...</td>\n",
       "      <td>1.042734</td>\n",
       "      <td>1.252503</td>\n",
       "      <td>5.196968</td>\n",
       "      <td>-0.976450</td>\n",
       "      <td>-3.194811</td>\n",
       "      <td>1.404666</td>\n",
       "      <td>-0.177189</td>\n",
       "      <td>3.253450</td>\n",
       "      <td>-0.105140</td>\n",
       "      <td>-1.102063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "id                                                                      \n",
       "0    -2.259876   1.259865  -0.908350  -1.853991   0.385436  -1.105350   \n",
       "1    -2.529047   1.120999  -4.326973  -0.893663   0.598676  -0.352028   \n",
       "2     0.012222  -0.177944  -1.297251  -0.147131  -1.011613   1.365393   \n",
       "3    -0.089448  -2.826220   3.938348  -1.599376   1.190689   2.446367   \n",
       "4     0.318715   2.223043  -1.566211   1.067817   1.312906  -1.992862   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "995   3.541143  -0.192635   0.243902   1.083794   1.907259   3.304881   \n",
       "996   0.791110   1.798410  -1.411891  -0.161851   3.359425   2.244013   \n",
       "997   0.473525  -0.185459   0.951580   0.334926  -1.087654  -0.260079   \n",
       "998  -1.023864  -1.582119  -2.382119   3.359051   0.936478  -1.220890   \n",
       "999  -0.091433  -0.777077   2.764774   1.293155   0.918216  -0.588605   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  ...  feature_26  feature_27  \\\n",
       "id                                                ...                           \n",
       "0     1.991604  -0.975180  -0.610046   -2.077745  ...    0.582761   -0.758695   \n",
       "1    -1.222735  -0.078255  -0.574625    0.527005  ...   -0.304697   -0.329206   \n",
       "2    -1.620815  -0.499335   1.425400    2.320669  ...    0.359054    1.573846   \n",
       "3     1.675493  -0.457380   0.497983    1.533945  ...    1.337707   -1.240190   \n",
       "4     1.511649   0.590657  -0.708409   -3.091795  ...   -0.455276   -1.605761   \n",
       "..         ...        ...        ...         ...  ...         ...         ...   \n",
       "995  -0.295063  -0.435150   2.418466   -0.104594  ...   -0.263075   -0.052187   \n",
       "996  -1.182066  -1.770667  -0.640782   -3.740578  ...   -1.021918   -1.518739   \n",
       "997  -2.475909   0.493006   0.840025   -0.238076  ...    0.666892    0.124335   \n",
       "998   2.902376   0.158716  -0.128563    6.958211  ...   -0.091385    3.584082   \n",
       "999  -2.711436   2.575381   0.436611    1.012601  ...    1.042734    1.252503   \n",
       "\n",
       "     feature_28  feature_29  feature_30  feature_31  feature_32  feature_33  \\\n",
       "id                                                                            \n",
       "0     -1.753105    0.902077    0.079239    0.731773    2.401233   -0.056351   \n",
       "1      1.524611    0.662350   -0.898723    2.235237    0.596613   -0.914091   \n",
       "2      2.876398    0.634198   -0.699412   -0.424788    0.146164   -0.335836   \n",
       "3      0.553714    1.999368   -0.796620    1.076099    0.907682    2.919312   \n",
       "4     -3.269420   -0.527665    1.735746    0.167339    0.612655   -0.788140   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "995   -2.089612   -0.257640    3.437469   -1.640569   -0.345324   -1.990600   \n",
       "996   -0.928221   -0.176392    1.954623   -1.229088    1.287181   -0.600885   \n",
       "997    0.803261   -2.884849   -1.499423   -0.120510    0.685016    0.393198   \n",
       "998    1.706777    4.733215    3.194062    0.854198    0.635615   -3.252377   \n",
       "999    5.196968   -0.976450   -3.194811    1.404666   -0.177189    3.253450   \n",
       "\n",
       "     feature_34  feature_35  \n",
       "id                           \n",
       "0     -1.691496    0.692310  \n",
       "1     -0.821066   -1.109875  \n",
       "2     -0.473143   -0.192600  \n",
       "3     -0.316218    2.976172  \n",
       "4     -0.006253    0.208321  \n",
       "..          ...         ...  \n",
       "995    1.105713    1.017189  \n",
       "996   -1.487602    1.171236  \n",
       "997   -0.495217   -2.488886  \n",
       "998   -1.140647   -1.304214  \n",
       "999   -0.105140   -1.102063  \n",
       "\n",
       "[1000 rows x 35 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the test dataset\n",
    "df_test = pd.read_csv('test_data.csv', index_col=0)\n",
    "\n",
    "# Visualize the dataset\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.434444</td>\n",
       "      <td>0.657116</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.030707</td>\n",
       "      <td>0.265214</td>\n",
       "      <td>-0.538569</td>\n",
       "      <td>1.097759</td>\n",
       "      <td>-1.135797</td>\n",
       "      <td>-0.730989</td>\n",
       "      <td>-0.734794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>-0.635366</td>\n",
       "      <td>-0.594249</td>\n",
       "      <td>0.264870</td>\n",
       "      <td>0.088284</td>\n",
       "      <td>0.614043</td>\n",
       "      <td>2.414138</td>\n",
       "      <td>-0.180984</td>\n",
       "      <td>-1.785150</td>\n",
       "      <td>0.208651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.578435</td>\n",
       "      <td>0.585199</td>\n",
       "      <td>-1.650163</td>\n",
       "      <td>-0.523867</td>\n",
       "      <td>0.477534</td>\n",
       "      <td>-0.240718</td>\n",
       "      <td>-0.688568</td>\n",
       "      <td>-0.200187</td>\n",
       "      <td>-0.695890</td>\n",
       "      <td>0.062179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439690</td>\n",
       "      <td>-0.398324</td>\n",
       "      <td>0.309225</td>\n",
       "      <td>0.189487</td>\n",
       "      <td>-0.430704</td>\n",
       "      <td>2.038327</td>\n",
       "      <td>0.558815</td>\n",
       "      <td>-0.589524</td>\n",
       "      <td>-0.902993</td>\n",
       "      <td>-0.776633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.219003</td>\n",
       "      <td>-0.087507</td>\n",
       "      <td>-0.619533</td>\n",
       "      <td>-0.129864</td>\n",
       "      <td>-1.125807</td>\n",
       "      <td>0.438322</td>\n",
       "      <td>-0.909796</td>\n",
       "      <td>-0.639429</td>\n",
       "      <td>1.285976</td>\n",
       "      <td>0.610984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224849</td>\n",
       "      <td>0.652005</td>\n",
       "      <td>0.681833</td>\n",
       "      <td>0.180634</td>\n",
       "      <td>-0.324933</td>\n",
       "      <td>-0.481608</td>\n",
       "      <td>0.095710</td>\n",
       "      <td>-0.314102</td>\n",
       "      <td>-0.550384</td>\n",
       "      <td>-0.275144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.273390</td>\n",
       "      <td>-1.459017</td>\n",
       "      <td>1.161476</td>\n",
       "      <td>-0.896326</td>\n",
       "      <td>1.066993</td>\n",
       "      <td>0.865722</td>\n",
       "      <td>0.922084</td>\n",
       "      <td>-0.595664</td>\n",
       "      <td>0.366979</td>\n",
       "      <td>0.370271</td>\n",
       "      <td>...</td>\n",
       "      <td>1.204664</td>\n",
       "      <td>-0.901112</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>0.609921</td>\n",
       "      <td>-0.376520</td>\n",
       "      <td>0.940235</td>\n",
       "      <td>0.878624</td>\n",
       "      <td>1.236319</td>\n",
       "      <td>-0.391345</td>\n",
       "      <td>1.457275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.055046</td>\n",
       "      <td>1.155935</td>\n",
       "      <td>-0.711026</td>\n",
       "      <td>0.511358</td>\n",
       "      <td>1.188682</td>\n",
       "      <td>-0.889477</td>\n",
       "      <td>0.831030</td>\n",
       "      <td>0.497575</td>\n",
       "      <td>-0.828460</td>\n",
       "      <td>-1.045061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.590447</td>\n",
       "      <td>-1.102877</td>\n",
       "      <td>-1.012207</td>\n",
       "      <td>-0.184722</td>\n",
       "      <td>0.967363</td>\n",
       "      <td>0.079335</td>\n",
       "      <td>0.575307</td>\n",
       "      <td>-0.529534</td>\n",
       "      <td>-0.077204</td>\n",
       "      <td>-0.055954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.668766</td>\n",
       "      <td>-0.095116</td>\n",
       "      <td>-0.095275</td>\n",
       "      <td>0.519791</td>\n",
       "      <td>1.780471</td>\n",
       "      <td>1.205164</td>\n",
       "      <td>-0.173027</td>\n",
       "      <td>-0.572476</td>\n",
       "      <td>2.270025</td>\n",
       "      <td>-0.131071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398018</td>\n",
       "      <td>-0.245432</td>\n",
       "      <td>-0.687004</td>\n",
       "      <td>-0.099811</td>\n",
       "      <td>1.870437</td>\n",
       "      <td>-1.633360</td>\n",
       "      <td>-0.409587</td>\n",
       "      <td>-1.102263</td>\n",
       "      <td>1.049741</td>\n",
       "      <td>0.386268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.197657</td>\n",
       "      <td>0.936022</td>\n",
       "      <td>-0.658531</td>\n",
       "      <td>-0.137633</td>\n",
       "      <td>3.226370</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>-0.665967</td>\n",
       "      <td>-1.965594</td>\n",
       "      <td>-0.761447</td>\n",
       "      <td>-1.243569</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.157761</td>\n",
       "      <td>-1.054848</td>\n",
       "      <td>-0.366876</td>\n",
       "      <td>-0.074262</td>\n",
       "      <td>1.083517</td>\n",
       "      <td>-1.243550</td>\n",
       "      <td>1.268785</td>\n",
       "      <td>-0.440344</td>\n",
       "      <td>-1.578509</td>\n",
       "      <td>0.470487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.027768</td>\n",
       "      <td>-0.091399</td>\n",
       "      <td>0.145458</td>\n",
       "      <td>0.124555</td>\n",
       "      <td>-1.201519</td>\n",
       "      <td>-0.204363</td>\n",
       "      <td>-1.385003</td>\n",
       "      <td>0.395713</td>\n",
       "      <td>0.705916</td>\n",
       "      <td>-0.171912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533053</td>\n",
       "      <td>-0.148006</td>\n",
       "      <td>0.110391</td>\n",
       "      <td>-0.925955</td>\n",
       "      <td>-0.749485</td>\n",
       "      <td>-0.193354</td>\n",
       "      <td>0.649702</td>\n",
       "      <td>0.033136</td>\n",
       "      <td>-0.572756</td>\n",
       "      <td>-1.530560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.773249</td>\n",
       "      <td>-0.814713</td>\n",
       "      <td>-0.988576</td>\n",
       "      <td>1.720621</td>\n",
       "      <td>0.813878</td>\n",
       "      <td>-0.584251</td>\n",
       "      <td>1.603908</td>\n",
       "      <td>0.047005</td>\n",
       "      <td>-0.253878</td>\n",
       "      <td>2.029927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226124</td>\n",
       "      <td>1.761491</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>1.469598</td>\n",
       "      <td>1.741266</td>\n",
       "      <td>0.730020</td>\n",
       "      <td>0.598912</td>\n",
       "      <td>-1.703245</td>\n",
       "      <td>-1.226880</td>\n",
       "      <td>-0.882881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.274452</td>\n",
       "      <td>-0.397791</td>\n",
       "      <td>0.762258</td>\n",
       "      <td>0.630287</td>\n",
       "      <td>0.795695</td>\n",
       "      <td>-0.334257</td>\n",
       "      <td>-1.515894</td>\n",
       "      <td>2.567902</td>\n",
       "      <td>0.306164</td>\n",
       "      <td>0.210756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909341</td>\n",
       "      <td>0.474650</td>\n",
       "      <td>1.321478</td>\n",
       "      <td>-0.325845</td>\n",
       "      <td>-1.649197</td>\n",
       "      <td>1.251498</td>\n",
       "      <td>-0.236728</td>\n",
       "      <td>1.395469</td>\n",
       "      <td>-0.177424</td>\n",
       "      <td>-0.772362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -1.434444  0.657116 -0.487240 -1.030707  0.265214 -0.538569  1.097759   \n",
       "1   -1.578435  0.585199 -1.650163 -0.523867  0.477534 -0.240718 -0.688568   \n",
       "2   -0.219003 -0.087507 -0.619533 -0.129864 -1.125807  0.438322 -0.909796   \n",
       "3   -0.273390 -1.459017  1.161476 -0.896326  1.066993  0.865722  0.922084   \n",
       "4   -0.055046  1.155935 -0.711026  0.511358  1.188682 -0.889477  0.831030   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  1.668766 -0.095116 -0.095275  0.519791  1.780471  1.205164 -0.173027   \n",
       "996  0.197657  0.936022 -0.658531 -0.137633  3.226370  0.785714 -0.665967   \n",
       "997  0.027768 -0.091399  0.145458  0.124555 -1.201519 -0.204363 -1.385003   \n",
       "998 -0.773249 -0.814713 -0.988576  1.720621  0.813878 -0.584251  1.603908   \n",
       "999 -0.274452 -0.397791  0.762258  0.630287  0.795695 -0.334257 -1.515894   \n",
       "\n",
       "           7         8         9   ...        25        26        27  \\\n",
       "0   -1.135797 -0.730989 -0.734794  ...  0.448822 -0.635366 -0.594249   \n",
       "1   -0.200187 -0.695890  0.062179  ... -0.439690 -0.398324  0.309225   \n",
       "2   -0.639429  1.285976  0.610984  ...  0.224849  0.652005  0.681833   \n",
       "3   -0.595664  0.366979  0.370271  ...  1.204664 -0.901112  0.041606   \n",
       "4    0.497575 -0.828460 -1.045061  ... -0.590447 -1.102877 -1.012207   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995 -0.572476  2.270025 -0.131071  ... -0.398018 -0.245432 -0.687004   \n",
       "996 -1.965594 -0.761447 -1.243569  ... -1.157761 -1.054848 -0.366876   \n",
       "997  0.395713  0.705916 -0.171912  ...  0.533053 -0.148006  0.110391   \n",
       "998  0.047005 -0.253878  2.029927  ... -0.226124  1.761491  0.359438   \n",
       "999  2.567902  0.306164  0.210756  ...  0.909341  0.474650  1.321478   \n",
       "\n",
       "           28        29        30        31        32        33        34  \n",
       "0    0.264870  0.088284  0.614043  2.414138 -0.180984 -1.785150  0.208651  \n",
       "1    0.189487 -0.430704  2.038327  0.558815 -0.589524 -0.902993 -0.776633  \n",
       "2    0.180634 -0.324933 -0.481608  0.095710 -0.314102 -0.550384 -0.275144  \n",
       "3    0.609921 -0.376520  0.940235  0.878624  1.236319 -0.391345  1.457275  \n",
       "4   -0.184722  0.967363  0.079335  0.575307 -0.529534 -0.077204 -0.055954  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995 -0.099811  1.870437 -1.633360 -0.409587 -1.102263  1.049741  0.386268  \n",
       "996 -0.074262  1.083517 -1.243550  1.268785 -0.440344 -1.578509  0.470487  \n",
       "997 -0.925955 -0.749485 -0.193354  0.649702  0.033136 -0.572756 -1.530560  \n",
       "998  1.469598  1.741266  0.730020  0.598912 -1.703245 -1.226880 -0.882881  \n",
       "999 -0.325845 -1.649197  1.251498 -0.236728  1.395469 -0.177424 -0.772362  \n",
       "\n",
       "[1000 rows x 35 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the test dataset\n",
    "scaler = StandardScaler()\n",
    "df_test_normalized = pd.DataFrame(scaler.fit_transform(df_test))\n",
    "df_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 3, 1, 3, 2, 3, 1, 3, 2, 0, 3, 0, 0, 2, 2, 3, 0, 2, 2, 3,\n",
       "       0, 0, 1, 1, 2, 2, 0, 3, 1, 2, 1, 3, 2, 2, 2, 3, 0, 3, 0, 1, 3, 2,\n",
       "       0, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 1, 0, 1, 2, 2, 3, 1, 2, 1, 2, 3,\n",
       "       3, 2, 0, 2, 1, 3, 2, 1, 1, 0, 0, 2, 3, 1, 0, 0, 3, 1, 1, 0, 1, 0,\n",
       "       0, 2, 3, 1, 1, 0, 1, 3, 3, 3, 0, 3, 0, 1, 0, 0, 2, 3, 3, 3, 2, 0,\n",
       "       2, 1, 1, 2, 3, 2, 2, 1, 0, 0, 3, 2, 3, 2, 0, 2, 0, 0, 1, 2, 2, 3,\n",
       "       3, 2, 3, 3, 3, 0, 1, 2, 0, 0, 3, 0, 2, 2, 1, 3, 2, 0, 2, 2, 1, 3,\n",
       "       0, 0, 3, 0, 0, 3, 1, 1, 0, 2, 2, 1, 1, 3, 0, 2, 2, 0, 1, 0, 2, 2,\n",
       "       2, 1, 1, 2, 3, 0, 1, 2, 2, 3, 2, 1, 2, 2, 1, 3, 1, 0, 0, 1, 2, 2,\n",
       "       3, 2, 2, 1, 1, 3, 1, 2, 0, 1, 2, 2, 3, 2, 1, 2, 3, 1, 1, 3, 1, 2,\n",
       "       1, 2, 1, 3, 1, 1, 0, 2, 1, 3, 2, 0, 3, 2, 3, 0, 0, 3, 2, 0, 2, 2,\n",
       "       1, 2, 1, 0, 1, 0, 1, 3, 0, 2, 2, 3, 3, 1, 0, 2, 0, 3, 3, 2, 0, 2,\n",
       "       0, 1, 3, 1, 0, 1, 1, 2, 2, 1, 3, 3, 2, 0, 0, 3, 3, 3, 0, 1, 2, 2,\n",
       "       0, 0, 1, 1, 2, 1, 0, 0, 0, 2, 2, 2, 2, 3, 3, 2, 1, 1, 0, 3, 3, 3,\n",
       "       2, 0, 0, 1, 3, 1, 1, 2, 3, 3, 1, 2, 3, 1, 1, 2, 3, 2, 3, 3, 0, 3,\n",
       "       2, 2, 3, 3, 0, 0, 2, 2, 0, 1, 2, 2, 1, 0, 2, 2, 3, 1, 2, 2, 2, 3,\n",
       "       3, 0, 1, 3, 3, 1, 2, 2, 3, 3, 1, 0, 3, 2, 2, 1, 3, 3, 1, 3, 3, 2,\n",
       "       2, 2, 2, 2, 0, 3, 0, 2, 1, 0, 3, 0, 3, 0, 2, 3, 0, 3, 3, 1, 0, 2,\n",
       "       2, 2, 1, 0, 2, 1, 1, 3, 2, 0, 3, 2, 2, 0, 1, 2, 0, 0, 1, 3, 0, 1,\n",
       "       3, 0, 3, 0, 0, 0, 0, 1, 3, 3, 2, 2, 0, 0, 3, 2, 0, 3, 3, 0, 2, 2,\n",
       "       3, 0, 2, 3, 2, 2, 3, 3, 1, 0, 3, 2, 2, 3, 1, 2, 1, 0, 2, 2, 1, 3,\n",
       "       2, 1, 0, 3, 0, 0, 0, 3, 2, 2, 3, 0, 0, 1, 3, 1, 3, 0, 2, 3, 1, 2,\n",
       "       3, 1, 3, 0, 2, 3, 0, 0, 3, 1, 2, 1, 2, 0, 2, 1, 0, 0, 1, 1, 1, 1,\n",
       "       3, 1, 2, 3, 2, 3, 0, 1, 2, 3, 1, 3, 1, 1, 2, 0, 3, 2, 3, 1, 3, 2,\n",
       "       2, 0, 0, 0, 0, 0, 3, 0, 2, 2, 1, 0, 0, 2, 2, 1, 0, 2, 2, 2, 3, 2,\n",
       "       2, 0, 3, 0, 3, 0, 0, 3, 2, 1, 2, 0, 2, 0, 3, 2, 2, 1, 3, 2, 2, 1,\n",
       "       3, 2, 0, 2, 0, 2, 1, 0, 2, 0, 3, 0, 3, 0, 1, 0, 0, 3, 0, 3, 0, 2,\n",
       "       1, 3, 1, 0, 0, 2, 0, 2, 3, 3, 2, 2, 2, 1, 0, 1, 3, 1, 0, 0, 2, 3,\n",
       "       0, 1, 3, 2, 0, 0, 3, 3, 3, 2, 3, 3, 2, 1, 0, 0, 0, 3, 2, 3, 1, 2,\n",
       "       3, 1, 1, 1, 2, 2, 2, 1, 3, 0, 0, 2, 0, 2, 3, 0, 3, 1, 0, 1, 1, 3,\n",
       "       0, 0, 1, 0, 2, 1, 1, 1, 3, 2, 3, 2, 2, 3, 1, 0, 3, 0, 2, 1, 1, 2,\n",
       "       1, 2, 2, 3, 1, 1, 1, 2, 3, 2, 3, 0, 1, 1, 0, 3, 1, 0, 3, 1, 0, 3,\n",
       "       2, 1, 2, 1, 2, 2, 3, 0, 1, 2, 3, 0, 1, 3, 3, 3, 0, 0, 2, 0, 3, 1,\n",
       "       2, 1, 0, 1, 0, 2, 1, 3, 0, 1, 1, 1, 3, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       3, 1, 2, 0, 2, 0, 2, 3, 2, 0, 0, 0, 1, 2, 3, 3, 0, 3, 3, 2, 0, 3,\n",
       "       2, 0, 3, 3, 2, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 3, 3, 0, 1, 1, 3, 1,\n",
       "       0, 1, 3, 3, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0,\n",
       "       0, 3, 0, 3, 2, 2, 3, 3, 2, 3, 2, 0, 3, 1, 3, 2, 3, 1, 2, 1, 1, 2,\n",
       "       3, 0, 0, 3, 2, 0, 0, 1, 2, 2, 3, 3, 1, 1, 3, 3, 0, 1, 0, 2, 2, 2,\n",
       "       1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 3, 2, 2, 1, 0, 2, 1, 2, 0, 0, 3, 0,\n",
       "       3, 2, 3, 0, 3, 0, 2, 0, 1, 2, 0, 3, 2, 3, 0, 1, 3, 3, 2, 2, 3, 0,\n",
       "       0, 1, 3, 3, 2, 0, 2, 2, 0, 2, 3, 3, 0, 1, 0, 1, 1, 0, 2, 2, 0, 3,\n",
       "       3, 1, 2, 2, 0, 1, 0, 2, 0, 1, 2, 3, 3, 2, 1, 1, 0, 0, 0, 0, 1, 2,\n",
       "       2, 0, 1, 3, 3, 0, 3, 0, 2, 3, 1, 3, 1, 1, 3, 2, 2, 0, 1, 1, 2, 1,\n",
       "       2, 1, 0, 0, 0, 1, 2, 3, 2, 3, 0, 0, 3, 1, 2, 0, 1, 3, 2, 1, 2, 1,\n",
       "       1, 3, 2, 3, 2, 1, 1, 2, 2, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best features from df_test\n",
    "df_test_normalized_selected = df_test_normalized[important_features[:knn3_best_num_features]]\n",
    "\n",
    "# Use the best model to predict the test set results\n",
    "df_test_predictions = knn3_final.predict(df_test_normalized_selected)\n",
    "df_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predictions = np.array(df_test_predictions)\n",
    "\n",
    "# Save to a txt file\n",
    "np.savetxt('test_predictions.txt', df_test_predictions, fmt='%d', newline='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:CadetBlue\">Extra: K-NN and PCA visualization</span>\n",
    "Just for the sake of curiosity we take the predictions on the labelled test dataset, and then we graphically visualize the mislabelled points after performing a PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best number of features\n",
    "selected_features = important_features[:knn3_best_num_features]\n",
    "\n",
    "# Use the best KNN model to make predictions on the test set\n",
    "knn3_test_predictions = knn3_final.predict(X_test[selected_features])\n",
    "\n",
    "# Perform PCA on the test data\n",
    "X_test_pca = pca.fit_transform(X_test[selected_features])\n",
    "\n",
    "# Create a boolean array that indicates whether each prediction was correct\n",
    "correct_predictions = (knn3_test_predictions == y_test)\n",
    "\n",
    "# Plot the PCA-transformed test data\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], X_test_pca[:, 2], \n",
    "                     c=['green' if correct else 'red' for correct in correct_predictions])\n",
    "\n",
    "# Create a legend for the colors\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"Predictions\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "plt.title('PCA of Test Data (Green = Correct Prediction, Red = Incorrect Prediction)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be that the errors are randomly distributed, this could be due to the fact the we lost information in the PCA, but we could argue that the dataset itself wasn't very suited for this kind of classifiers.    \n",
    "We are satisfied with our work, we managed to get almost 70% accuracy on the labelled test dataset, which we believe is satisfactory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
